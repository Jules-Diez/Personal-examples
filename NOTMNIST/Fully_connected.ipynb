{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " - [Load](#Load)\n",
    " - [Simple Network](#Simple-Network)\n",
    " - [Deep fully connected network](#Deep-fully-connected-network)\n",
    " - [Deep fully connected network with Tensorboard (coming soon)](#Simple-Convolutional-Network-with-Tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The preparation of the dataset is done in an other [notebook]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I tried 2 sizes for the validation set:\n",
    " - 60 000 that enables me and my GPU with 4Go of ram to deal with the validation set without using batchs\n",
    " - 80 000 where I need to put the validation set into a set of batchs for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (469090, 28, 28) (469090,)\n",
      "Validation set (60000, 28, 28) (60000,)\n",
      "Test set (18720, 28, 28) (18720,)\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "data_root = '.\\Data\\\\notmnist\\\\' # Change me to store data elsewhere\n",
    "pickle_file = 'notMNIST_valid_60k.pickle'\n",
    "try:\n",
    "    pickle_file = os.path.join(data_root, pickle_file)\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        train_dataset = save['train_dataset']\n",
    "        train_labels = save['train_labels']\n",
    "        valid_dataset = save['valid_dataset']\n",
    "        valid_labels = save['valid_labels']\n",
    "        test_dataset = save['test_dataset']\n",
    "        test_labels = save['test_labels']\n",
    "        del save  # hint to help gc free up memory\n",
    "        print('Training set', train_dataset.shape, train_labels.shape)\n",
    "        print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "        print('Test set', test_dataset.shape, test_labels.shape)\n",
    "except Exception as e:\n",
    "    print('Unable to load data from', pickle_file, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data set parameters \n",
    "image_size = 28  # Pixel width and height.\n",
    "n_input = image_size*image_size # MNIST data input\n",
    "num_labels = 10 # Number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (469090, 784) (469090, 10)\n",
      "Validation set (60000, 784) (60000, 10)\n",
      "Test set (18720, 784) (18720, 10)\n"
     ]
    }
   ],
   "source": [
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Simple Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Simple neural network with:\n",
    " - 1 hidden layer\n",
    " - Relu activation for hidden unit\n",
    " - Softmax loss\n",
    " - L2 regularization\n",
    " - Dropout\n",
    " - SGB with fixed learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This implementation is not using correctly placeholders but is straightforward and try to give a good insight of the mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters of the model\n",
    "batch_size = 128\n",
    "size_hidden_node = 1024\n",
    "learning_rate = 0.5\n",
    "beta_regul = 1e-3\n",
    "dropout_rate = 1 # Not necessary here\n",
    "size_train = train_labels.shape[0]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Constant for valid and test\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Placeholder for training\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    # Layer 1 - this is our hidden layer\n",
    "    weights_layer1 = tf.Variable(tf.truncated_normal([image_size * image_size, size_hidden_node]))\n",
    "    biases_layer1 = tf.Variable(tf.zeros([size_hidden_node]))\n",
    "    out_layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_layer1) + biases_layer1)\n",
    "    out_layer1_dropout = tf.nn.dropout(out_layer1, dropout_rate) \n",
    "    \n",
    "    # Layer 2\n",
    "    weights_layer2 = tf.Variable(tf.truncated_normal([size_hidden_node, num_labels]))\n",
    "    biases_layer2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    out_layer2 = tf.matmul(out_layer1_dropout, weights_layer2) + biases_layer2\n",
    "    \n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=out_layer2))\n",
    "    regularization = tf.nn.l2_loss(weights_layer1) + tf.nn.l2_loss(weights_layer2)\n",
    "    reg_loss = loss + beta_regul * regularization\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(reg_loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(out_layer2)\n",
    "    \n",
    "    valid_out_layer1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_layer1) + biases_layer1)\n",
    "    valid_out_layer2 = tf.matmul(valid_out_layer1, weights_layer2) + biases_layer2\n",
    "    valid_prediction = tf.nn.softmax(valid_out_layer2)\n",
    "    \n",
    "    test_out_layer1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_layer1) + biases_layer1)\n",
    "    test_out_layer2 = tf.matmul(test_out_layer1, weights_layer2) + biases_layer2\n",
    "    test_prediction = tf.nn.softmax(test_out_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "------Epoch n째1 ------\n",
      "\n",
      "Minibatch loss at step 0: 322.194519\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 30.3%\n",
      "\n",
      "Minibatch loss at step 2000: 0.839249\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.6%\n",
      "\n",
      "\n",
      "------Epoch n째2 ------\n",
      "\n",
      "Minibatch loss at step 4000: 0.497808\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.3%\n",
      "\n",
      "Minibatch loss at step 6000: 0.551060\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.1%\n",
      "\n",
      "\n",
      "------Epoch n째3 ------\n",
      "\n",
      "Minibatch loss at step 8000: 0.311141\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.3%\n",
      "\n",
      "Minibatch loss at step 10000: 0.351788\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "\n",
      "\n",
      "------Epoch n째4 ------\n",
      "\n",
      "Minibatch loss at step 12000: 0.327644\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.4%\n",
      "\n",
      "Minibatch loss at step 14000: 0.344216\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.1%\n",
      "\n",
      "\n",
      "------Epoch n째5 ------\n",
      "\n",
      "Minibatch loss at step 16000: 0.434843\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "\n",
      "Minibatch loss at step 18000: 0.347391\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.8%\n",
      "\n",
      "\n",
      "------Epoch n째6 ------\n",
      "\n",
      "Minibatch loss at step 20000: 0.336459\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.6%\n",
      "\n",
      "\n",
      "------Epoch n째7 ------\n",
      "\n",
      "Minibatch loss at step 22000: 0.341784\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.5%\n",
      "\n",
      "Minibatch loss at step 24000: 0.303143\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.9%\n",
      "\n",
      "\n",
      "------Epoch n째8 ------\n",
      "\n",
      "Minibatch loss at step 26000: 0.362766\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.7%\n",
      "\n",
      "Minibatch loss at step 28000: 0.322879\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.2%\n",
      "\n",
      "\n",
      "------Epoch n째9 ------\n",
      "\n",
      "Minibatch loss at step 30000: 0.325473\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.5%\n",
      "\n",
      "Minibatch loss at step 32000: 0.575603\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.4%\n",
      "\n",
      "\n",
      "------Epoch n째10 ------\n",
      "\n",
      "Minibatch loss at step 34000: 0.300804\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "\n",
      "Minibatch loss at step 36000: 0.317832\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.5%\n",
      "\n",
      "Test accuracy: 94.6%\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "n_step_one_epoch = int(size_train / batch_size) # Some images will not be used\n",
    "num_steps = int(n_epoch * n_step_one_epoch)\n",
    "t1 = time.time()   \n",
    "\n",
    "num_epoch = 1\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        if ( step % n_step_one_epoch == 0 ):\n",
    "            print('\\n------Epoch n째%d ------\\n' % num_epoch)\n",
    "            num_epoch +=1\n",
    "        # Simple offset for batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Batch creation\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Feed dictionary\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        # Run optimizer, loss and train_prediction with the feeded batch\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 2000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print()\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deep fully connected network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's build a more advanced fully connected network with re-usable code made of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Deep neural network with:\n",
    " - 3 hidden layers\n",
    " - Relu activation for hidden layers\n",
    " - Softmax loss\n",
    " - L2 regularization\n",
    " - RMSProp with fixed decay learning rate and momentum\n",
    " - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The network is not really tuned, to do it's easier with tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape,stddev,name):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev, name=name)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape, value, name):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(value, shape=shape, name=name)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nn_layer(input_tensor, input_dim, output_dim, keep_prob, layer_name, weight_stddev, bias_value ,act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer. \n",
    "    It does a matrix multiply, bias add, and then an activation function.\n",
    "    \"\"\"\n",
    "    weights = weight_variable([input_dim, output_dim], weight_stddev, 'weight_'+layer_name)\n",
    "    tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights)\n",
    "    biases = bias_variable([output_dim], bias_value, 'bias_'+layer_name)\n",
    "    preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "    activations = act(preactivate, name='activation')\n",
    "    dropped = tf.nn.dropout(activations, keep_prob)\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loss_function(labels, output_layer, coef_reg, reg='l2', coef_reg2=1e-3):\n",
    "    \"\"\" Loss function with softmax implementing L1,L2 and L1+L2 regularization\"\"\"\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=output_layer))\n",
    "    if (reg == 'l1'):\n",
    "        regularizer = tf.contrib.layers.l1_regularizer(scale=coef_reg)\n",
    "    elif (reg == 'l2'):\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=coef_reg)\n",
    "    else:\n",
    "        l1 = regularizer = tf.contrib.layers.l1_regularizer(scale=coef_reg)\n",
    "        l2 = regularizer = tf.contrib.layers.l2_regularizer(scale=coef_reg2)\n",
    "        regularizer = tf.contrib.layers.sum_regularizer([l1,l2])\n",
    "        \n",
    "    reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "    reg_loss = cross_entropy + reg_term\n",
    "    return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feed_dict(dataset, dropout_rate=0.5):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if (dataset == 'train'):\n",
    "        offset = (step * batch_size) % (size_train - batch_size)\n",
    "        data_batch = train_dataset[offset:(offset + batch_size), :]\n",
    "        label_batch = train_labels[offset:(offset + batch_size), :]\n",
    "        keep_prob = dropout_rate\n",
    "    elif (dataset == 'validation'):\n",
    "        data_batch = valid_dataset\n",
    "        label_batch = valid_labels\n",
    "        keep_prob = 1\n",
    "    elif (dataset == 'test'):\n",
    "        data_batch = test_dataset\n",
    "        label_batch = test_labels\n",
    "        keep_prob = 1\n",
    "    return {x: data_batch, y_: label_batch, k: keep_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data set parameters \n",
    "image_size = 28  # Pixel width and height.\n",
    "n_input = image_size*image_size # MNIST data input\n",
    "num_labels = 10 # Number of labels\n",
    "size_train = train_labels.shape[0]\n",
    "\n",
    "# Parameters of the model\n",
    "batch_size = 300\n",
    "size_hidden_layer = [1024,300,60]\n",
    "std_weights = [np.sqrt(2.0/size_hidden_layer[0]),np.sqrt(2.0/size_hidden_layer[1]),\n",
    "               np.sqrt(2.0/size_hidden_layer[2]),0.01]\n",
    "bias_value = 0.1\n",
    "learning_rate = 1e-4\n",
    "regul = 8e-4\n",
    "decay_rate = 0.95\n",
    "dropout_rate = 0.5\n",
    "momentum = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Count the number of steps taken\n",
    "    global_step = tf.Variable(0) \n",
    "    \n",
    "    # Placeholder for train, valid and test and the dropout rate\n",
    "    x = tf.placeholder(tf.float32, shape=[None, image_size * image_size], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, num_labels], name='y-input')\n",
    "    k = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Define the model\n",
    "    layer_1 = nn_layer(x, n_input, size_hidden_layer[0], k, \n",
    "                       'layer_1', std_weights[0], bias_value, tf.nn.relu)\n",
    "    layer_2 = nn_layer(layer_1, size_hidden_layer[0], size_hidden_layer[1], k, \n",
    "                       'layer_2', std_weights[1], bias_value, tf.nn.relu)\n",
    "    layer_3 = nn_layer(layer_2, size_hidden_layer[1], size_hidden_layer[2], k, \n",
    "                       'layer_3', std_weights[2], bias_value, tf.nn.relu)\n",
    "    y = nn_layer(layer_3, size_hidden_layer[2], num_labels, k, \n",
    "                 'layer_4', std_weights[3], bias_value, tf.identity)       \n",
    "    \n",
    "    # Loss function - Cross entropy + regularization   \n",
    "    reg_loss = loss_function(y_, y, regul, reg='l2')\n",
    "\n",
    "    # Optimizer    \n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=decay_rate, momentum=momentum\n",
    "                                         ).minimize(reg_loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the train, valid, and test.\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy = tf.cast(accuracy, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "------Epoch n째1 ------\n",
      "\n",
      "Minibatch loss at step 0: 3.604520\n",
      "Minibatch accuracy: 12.00% \n",
      "Validation accuracy: 10.38%\n",
      "\n",
      "Minibatch loss at step 1000: 1.203882\n",
      "Minibatch accuracy: 88.00% \n",
      "Validation accuracy: 86.89%\n",
      "\n",
      "\n",
      "------Epoch n째2 ------\n",
      "\n",
      "Minibatch loss at step 2000: 0.792411\n",
      "Minibatch accuracy: 88.33% \n",
      "Validation accuracy: 88.48%\n",
      "\n",
      "Minibatch loss at step 3000: 0.605802\n",
      "Minibatch accuracy: 89.67% \n",
      "Validation accuracy: 89.12%\n",
      "\n",
      "\n",
      "------Epoch n째3 ------\n",
      "\n",
      "Minibatch loss at step 4000: 0.615607\n",
      "Minibatch accuracy: 89.00% \n",
      "Validation accuracy: 89.46%\n",
      "\n",
      "\n",
      "------Epoch n째4 ------\n",
      "\n",
      "Minibatch loss at step 5000: 0.499206\n",
      "Minibatch accuracy: 90.67% \n",
      "Validation accuracy: 89.93%\n",
      "\n",
      "Minibatch loss at step 6000: 0.380488\n",
      "Minibatch accuracy: 92.33% \n",
      "Validation accuracy: 89.91%\n",
      "\n",
      "\n",
      "------Epoch n째5 ------\n",
      "\n",
      "Minibatch loss at step 7000: 0.455447\n",
      "Minibatch accuracy: 91.00% \n",
      "Validation accuracy: 90.25%\n",
      "\n",
      "\n",
      "------Epoch n째6 ------\n",
      "\n",
      "Minibatch loss at step 8000: 0.403935\n",
      "Minibatch accuracy: 91.67% \n",
      "Validation accuracy: 90.48%\n",
      "\n",
      "Minibatch loss at step 9000: 0.434824\n",
      "Minibatch accuracy: 91.00% \n",
      "Validation accuracy: 90.01%\n",
      "\n",
      "\n",
      "------Epoch n째7 ------\n",
      "\n",
      "Minibatch loss at step 10000: 0.337160\n",
      "Minibatch accuracy: 92.67% \n",
      "Validation accuracy: 90.47%\n",
      "\n",
      "\n",
      "------Epoch n째8 ------\n",
      "\n",
      "Minibatch loss at step 11000: 0.347571\n",
      "Minibatch accuracy: 92.67% \n",
      "Validation accuracy: 90.30%\n",
      "\n",
      "Minibatch loss at step 12000: 0.377071\n",
      "Minibatch accuracy: 91.33% \n",
      "Validation accuracy: 90.65%\n",
      "\n",
      "\n",
      "------Epoch n째9 ------\n",
      "\n",
      "Minibatch loss at step 13000: 0.380352\n",
      "Minibatch accuracy: 91.33% \n",
      "Validation accuracy: 90.69%\n",
      "\n",
      "Minibatch loss at step 14000: 0.375906\n",
      "Minibatch accuracy: 90.67% \n",
      "Validation accuracy: 90.76%\n",
      "\n",
      "\n",
      "------Epoch n째10 ------\n",
      "\n",
      "Minibatch loss at step 15000: 0.323526\n",
      "Minibatch accuracy: 93.00% \n",
      "Validation accuracy: 90.86%\n",
      "\n",
      "\n",
      "------Epoch n째11 ------\n",
      "\n",
      "Minibatch loss at step 16000: 0.409600\n",
      "Minibatch accuracy: 91.33% \n",
      "Validation accuracy: 90.88%\n",
      "\n",
      "Minibatch loss at step 17000: 0.403045\n",
      "Minibatch accuracy: 91.67% \n",
      "Validation accuracy: 90.67%\n",
      "\n",
      "\n",
      "------Epoch n째12 ------\n",
      "\n",
      "Minibatch loss at step 18000: 0.377270\n",
      "Minibatch accuracy: 90.67% \n",
      "Validation accuracy: 90.73%\n",
      "\n",
      "\n",
      "------Epoch n째13 ------\n",
      "\n",
      "Minibatch loss at step 19000: 0.281158\n",
      "Minibatch accuracy: 95.00% \n",
      "Validation accuracy: 90.84%\n",
      "\n",
      "Minibatch loss at step 20000: 0.417360\n",
      "Minibatch accuracy: 90.67% \n",
      "Validation accuracy: 91.05%\n",
      "\n",
      "\n",
      "------Epoch n째14 ------\n",
      "\n",
      "Minibatch loss at step 21000: 0.334970\n",
      "Minibatch accuracy: 94.33% \n",
      "Validation accuracy: 91.07%\n",
      "\n",
      "\n",
      "------Epoch n째15 ------\n",
      "\n",
      "Minibatch loss at step 22000: 0.394568\n",
      "Minibatch accuracy: 92.00% \n",
      "Validation accuracy: 91.18%\n",
      "\n",
      "Minibatch loss at step 23000: 0.282566\n",
      "Minibatch accuracy: 95.00% \n",
      "Validation accuracy: 91.01%\n",
      "\n",
      "\n",
      "------Epoch n째16 ------\n",
      "\n",
      "Minibatch loss at step 24000: 0.279085\n",
      "Minibatch accuracy: 95.00% \n",
      "Validation accuracy: 90.99%\n",
      "\n",
      "Minibatch loss at step 25000: 0.341902\n",
      "Minibatch accuracy: 92.00% \n",
      "Validation accuracy: 90.99%\n",
      "\n",
      "\n",
      "------Epoch n째17 ------\n",
      "\n",
      "Minibatch loss at step 26000: 0.312704\n",
      "Minibatch accuracy: 93.67% \n",
      "Validation accuracy: 91.00%\n",
      "\n",
      "\n",
      "------Epoch n째18 ------\n",
      "\n",
      "Minibatch loss at step 27000: 0.326212\n",
      "Minibatch accuracy: 94.33% \n",
      "Validation accuracy: 91.16%\n",
      "\n",
      "Minibatch loss at step 28000: 0.305852\n",
      "Minibatch accuracy: 94.33% \n",
      "Validation accuracy: 90.37%\n",
      "\n",
      "\n",
      "------Epoch n째19 ------\n",
      "\n",
      "Minibatch loss at step 29000: 0.391679\n",
      "Minibatch accuracy: 92.33% \n",
      "Validation accuracy: 91.15%\n",
      "\n",
      "\n",
      "------Epoch n째20 ------\n",
      "\n",
      "Minibatch loss at step 30000: 0.290877\n",
      "Minibatch accuracy: 94.33% \n",
      "Validation accuracy: 91.25%\n",
      "\n",
      "Minibatch loss at step 31000: 0.328331\n",
      "Minibatch accuracy: 94.00% \n",
      "Validation accuracy: 91.05%\n",
      "\n",
      "\n",
      "------Epoch n째21 ------\n",
      "\n",
      "Minibatch loss at step 32000: 0.401215\n",
      "Minibatch accuracy: 92.67% \n",
      "Validation accuracy: 91.03%\n",
      "\n",
      "\n",
      "------Epoch n째22 ------\n",
      "\n",
      "Minibatch loss at step 33000: 0.318049\n",
      "Minibatch accuracy: 94.33% \n",
      "Validation accuracy: 91.02%\n",
      "\n",
      "Minibatch loss at step 34000: 0.382841\n",
      "Minibatch accuracy: 92.33% \n",
      "Validation accuracy: 91.32%\n",
      "\n",
      "\n",
      "------Epoch n째23 ------\n",
      "\n",
      "Minibatch loss at step 35000: 0.409402\n",
      "Minibatch accuracy: 92.00% \n",
      "Validation accuracy: 91.18%\n",
      "\n",
      "\n",
      "------Epoch n째24 ------\n",
      "\n",
      "Minibatch loss at step 36000: 0.363018\n",
      "Minibatch accuracy: 92.33% \n",
      "Validation accuracy: 91.33%\n",
      "\n",
      "Minibatch loss at step 37000: 0.352806\n",
      "Minibatch accuracy: 93.00% \n",
      "Validation accuracy: 91.37%\n",
      "\n",
      "\n",
      "------Epoch n째25 ------\n",
      "\n",
      "Minibatch loss at step 38000: 0.394135\n",
      "Minibatch accuracy: 92.00% \n",
      "Validation accuracy: 91.18%\n",
      "\n",
      "Minibatch loss at step 39000: 0.347137\n",
      "Minibatch accuracy: 92.33% \n",
      "Validation accuracy: 91.09%\n",
      "\n",
      "\n",
      "------Epoch n째26 ------\n",
      "\n",
      "Minibatch loss at step 40000: 0.293361\n",
      "Minibatch accuracy: 94.67% \n",
      "Validation accuracy: 91.39%\n",
      "\n",
      "\n",
      "------Epoch n째27 ------\n",
      "\n",
      "Minibatch loss at step 41000: 0.467670\n",
      "Minibatch accuracy: 89.67% \n",
      "Validation accuracy: 91.22%\n",
      "\n",
      "Minibatch loss at step 42000: 0.361376\n",
      "Minibatch accuracy: 92.33% \n",
      "Validation accuracy: 91.50%\n",
      "\n",
      "\n",
      "------Epoch n째28 ------\n",
      "\n",
      "Minibatch loss at step 43000: 0.315745\n",
      "Minibatch accuracy: 94.33% \n",
      "Validation accuracy: 91.39%\n",
      "\n",
      "\n",
      "------Epoch n째29 ------\n",
      "\n",
      "Minibatch loss at step 44000: 0.336697\n",
      "Minibatch accuracy: 94.00% \n",
      "Validation accuracy: 91.11%\n",
      "\n",
      "Minibatch loss at step 45000: 0.332694\n",
      "Minibatch accuracy: 94.67% \n",
      "Validation accuracy: 91.16%\n",
      "\n",
      "\n",
      "------Epoch n째30 ------\n",
      "\n",
      "Minibatch loss at step 46000: 0.320337\n",
      "Minibatch accuracy: 93.33% \n",
      "Validation accuracy: 91.35%\n",
      "\n",
      "\n",
      "------Epoch n째31 ------\n",
      "\n",
      "Minibatch loss at step 47000: 0.303562\n",
      "Minibatch accuracy: 95.33% \n",
      "Validation accuracy: 91.41%\n",
      "\n",
      "Minibatch loss at step 48000: 0.335995\n",
      "Minibatch accuracy: 93.33% \n",
      "Validation accuracy: 91.22%\n",
      "\n",
      "\n",
      "------Epoch n째32 ------\n",
      "\n",
      "Minibatch loss at step 49000: 0.353662\n",
      "Minibatch accuracy: 94.67% \n",
      "Validation accuracy: 91.42%\n",
      "\n",
      "Minibatch loss at step 50000: 0.333797\n",
      "Minibatch accuracy: 93.33% \n",
      "Validation accuracy: 91.33%\n",
      "\n",
      "\n",
      "------Epoch n째33 ------\n",
      "\n",
      "Minibatch loss at step 51000: 0.280603\n",
      "Minibatch accuracy: 96.33% \n",
      "Validation accuracy: 91.24%\n",
      "\n",
      "\n",
      "------Epoch n째34 ------\n",
      "\n",
      "Minibatch loss at step 52000: 0.353467\n",
      "Minibatch accuracy: 93.33% \n",
      "Validation accuracy: 91.41%\n",
      "\n",
      "Minibatch loss at step 53000: 0.356358\n",
      "Minibatch accuracy: 93.33% \n",
      "Validation accuracy: 91.30%\n",
      "\n",
      "\n",
      "------Epoch n째35 ------\n",
      "\n",
      "Minibatch loss at step 54000: 0.312223\n",
      "Minibatch accuracy: 94.67% \n",
      "Validation accuracy: 91.41%\n",
      "\n",
      "\n",
      "------Epoch n째36 ------\n",
      "\n",
      "Minibatch loss at step 55000: 0.301606\n",
      "Minibatch accuracy: 94.33% \n",
      "Validation accuracy: 91.42%\n",
      "\n",
      "Minibatch loss at step 56000: 0.297090\n",
      "Minibatch accuracy: 94.67% \n",
      "Validation accuracy: 91.21%\n",
      "\n",
      "\n",
      "------Epoch n째37 ------\n",
      "\n",
      "Minibatch loss at step 57000: 0.350529\n",
      "Minibatch accuracy: 93.00% \n",
      "Validation accuracy: 91.54%\n",
      "\n",
      "\n",
      "------Epoch n째38 ------\n",
      "\n",
      "Minibatch loss at step 58000: 0.268494\n",
      "Minibatch accuracy: 95.67% \n",
      "Validation accuracy: 91.06%\n",
      "\n",
      "Minibatch loss at step 59000: 0.357656\n",
      "Minibatch accuracy: 93.33% \n",
      "Validation accuracy: 91.41%\n",
      "\n",
      "\n",
      "------Epoch n째39 ------\n",
      "\n",
      "Minibatch loss at step 60000: 0.279058\n",
      "Minibatch accuracy: 95.33% \n",
      "Validation accuracy: 91.33%\n",
      "\n",
      "\n",
      "------Epoch n째40 ------\n",
      "\n",
      "Minibatch loss at step 61000: 0.301307\n",
      "Minibatch accuracy: 95.67% \n",
      "Validation accuracy: 91.12%\n",
      "\n",
      "Minibatch loss at step 62000: 0.337817\n",
      "Minibatch accuracy: 93.67% \n",
      "Validation accuracy: 91.40%\n",
      "\n",
      "Test accuracy: 96.21%\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 40\n",
    "n_step_one_epoch = int(size_train / batch_size)\n",
    "num_steps = int(n_epoch * n_step_one_epoch)\n",
    "interval_step_Train = 1000\n",
    "interval_step_Valid = 1000\n",
    "num_epoch = 1\n",
    "\n",
    "try:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            if ( step % n_step_one_epoch == 0 ):\n",
    "                print('\\n------Epoch n째%d ------\\n' % num_epoch)\n",
    "                num_epoch +=1\n",
    "            _, l, acc = session.run([optimizer, reg_loss, accuracy], feed_dict=feed_dict('train',dropout_rate))\n",
    "            # Run optimizer, loss and train_prediction with the feeded batch \n",
    "            if (step % interval_step_Train == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.2f%% \" % (100*acc))\n",
    "            if (step % interval_step_Valid == 0):\n",
    "                print('Validation accuracy: %.2f%%' % (100*accuracy.eval(feed_dict=feed_dict('validation'))))\n",
    "                print()\n",
    "        print('Test accuracy: %.2f%%' % (100*accuracy.eval(feed_dict=feed_dict('test'))))\n",
    "except Exception as e:\n",
    "    print('An error occur in step', step, ':', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deep fully connected network with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
