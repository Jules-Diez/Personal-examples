{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " - [Load](#Load)\n",
    " - [Simple Network](#Simple-Network)\n",
    " - [Deep fully connected network](#Deep-fully-connected-network)\n",
    " - [Deep fully connected network with Tensorboard](#Deep-fully-connected-network-with-Tensorboard)\n",
    " - [Deep fully connected network with Tensorboard using embedding visualization](#Deep-fully-connected-network-with-Tensorboard-using-embedding-visualization)\n",
    " - [Deep fully connected network with Tensorboard using embedding visualization and restore model to continue learning!](#Deep-fully-connected-network-with-Tensorboard-using-embedding-visualization-and-restore-model-to-continue-learning!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The preparation of the dataset is done in an other [notebook]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I tried 2 sizes for the validation set:\n",
    " - 60 000 that enables me and my GPU with 4Go of ram to deal with the validation set without using batchs\n",
    " - 80 000 where I need to put the validation set into a set of batchs for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (469090, 28, 28) (469090,)\n",
      "Validation set (60000, 28, 28) (60000,)\n",
      "Test set (18720, 28, 28) (18720,)\n"
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "data_root = '.\\Data\\\\notmnist\\\\' # Change me to store data elsewhere\n",
    "pickle_file = 'notMNIST_valid_60k.pickle'\n",
    "try:\n",
    "    pickle_file = os.path.join(data_root, pickle_file)\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        save = pickle.load(f)\n",
    "        train_dataset_raw = save['train_dataset']\n",
    "        train_labels_raw = save['train_labels']\n",
    "        valid_dataset_raw = save['valid_dataset']\n",
    "        valid_labels_raw = save['valid_labels']\n",
    "        test_dataset_raw = save['test_dataset']\n",
    "        test_labels_raw = save['test_labels']\n",
    "        del save  # hint to help gc free up memory\n",
    "        print('Training set', train_dataset_raw.shape, train_labels_raw.shape)\n",
    "        print('Validation set', valid_dataset_raw.shape, valid_labels_raw.shape)\n",
    "        print('Test set', test_dataset_raw.shape, test_labels_raw.shape)\n",
    "except Exception as e:\n",
    "    print('Unable to load data from', pickle_file, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data set parameters \n",
    "image_size = 28  # Pixel width and height.\n",
    "n_input = image_size*image_size # NOTMNIST data input\n",
    "num_labels = 10 # Number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (469090, 784) (469090, 10)\n",
      "Validation set (60000, 784) (60000, 10)\n",
      "Test set (18720, 784) (18720, 10)\n"
     ]
    }
   ],
   "source": [
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset_raw, train_labels_raw)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset_raw, valid_labels_raw)\n",
    "test_dataset, test_labels = reformat(test_dataset_raw, test_labels_raw)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Simple Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Simple neural network with:\n",
    " - 1 hidden layer\n",
    " - Relu activation for hidden unit\n",
    " - Softmax loss\n",
    " - L2 regularization\n",
    " - Dropout\n",
    " - SGB with fixed learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This implementation is not using correctly placeholders but is straightforward and try to give a good insight of the mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Parameters of the model\n",
    "batch_size = 128\n",
    "size_hidden_node = 1024\n",
    "learning_rate = 0.5\n",
    "beta_regul = 1e-3\n",
    "dropout_rate = 1 # Not necessary here\n",
    "size_train = train_labels.shape[0]\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Constant for valid and test\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Placeholder for training\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    # Layer 1 - this is our hidden layer\n",
    "    weights_layer1 = tf.Variable(tf.truncated_normal([image_size * image_size, size_hidden_node]))\n",
    "    biases_layer1 = tf.Variable(tf.zeros([size_hidden_node]))\n",
    "    out_layer1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_layer1) + biases_layer1)\n",
    "    out_layer1_dropout = tf.nn.dropout(out_layer1, dropout_rate) \n",
    "    \n",
    "    # Layer 2\n",
    "    weights_layer2 = tf.Variable(tf.truncated_normal([size_hidden_node, num_labels]))\n",
    "    biases_layer2 = tf.Variable(tf.zeros([num_labels]))\n",
    "    out_layer2 = tf.matmul(out_layer1_dropout, weights_layer2) + biases_layer2\n",
    "    \n",
    "    # Loss function\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=out_layer2))\n",
    "    regularization = tf.nn.l2_loss(weights_layer1) + tf.nn.l2_loss(weights_layer2)\n",
    "    reg_loss = loss + beta_regul * regularization\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(reg_loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(out_layer2)\n",
    "    \n",
    "    valid_out_layer1 = tf.nn.relu(tf.matmul(tf_valid_dataset, weights_layer1) + biases_layer1)\n",
    "    valid_out_layer2 = tf.matmul(valid_out_layer1, weights_layer2) + biases_layer2\n",
    "    valid_prediction = tf.nn.softmax(valid_out_layer2)\n",
    "    \n",
    "    test_out_layer1 = tf.nn.relu(tf.matmul(tf_test_dataset, weights_layer1) + biases_layer1)\n",
    "    test_out_layer2 = tf.matmul(test_out_layer1, weights_layer2) + biases_layer2\n",
    "    test_prediction = tf.nn.softmax(test_out_layer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "------Epoch n°1 ------\n",
      "\n",
      "Minibatch loss at step 0: 322.194519\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 30.3%\n",
      "\n",
      "Minibatch loss at step 2000: 0.839249\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.6%\n",
      "\n",
      "\n",
      "------Epoch n°2 ------\n",
      "\n",
      "Minibatch loss at step 4000: 0.497808\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.3%\n",
      "\n",
      "Minibatch loss at step 6000: 0.551060\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 88.1%\n",
      "\n",
      "\n",
      "------Epoch n°3 ------\n",
      "\n",
      "Minibatch loss at step 8000: 0.311141\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.3%\n",
      "\n",
      "Minibatch loss at step 10000: 0.351788\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 88.4%\n",
      "\n",
      "\n",
      "------Epoch n°4 ------\n",
      "\n",
      "Minibatch loss at step 12000: 0.327644\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 88.4%\n",
      "\n",
      "Minibatch loss at step 14000: 0.344216\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.1%\n",
      "\n",
      "\n",
      "------Epoch n°5 ------\n",
      "\n",
      "Minibatch loss at step 16000: 0.434843\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 88.6%\n",
      "\n",
      "Minibatch loss at step 18000: 0.347391\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.8%\n",
      "\n",
      "\n",
      "------Epoch n°6 ------\n",
      "\n",
      "Minibatch loss at step 20000: 0.336459\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.6%\n",
      "\n",
      "\n",
      "------Epoch n°7 ------\n",
      "\n",
      "Minibatch loss at step 22000: 0.341784\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.5%\n",
      "\n",
      "Minibatch loss at step 24000: 0.303143\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.9%\n",
      "\n",
      "\n",
      "------Epoch n°8 ------\n",
      "\n",
      "Minibatch loss at step 26000: 0.362766\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 88.7%\n",
      "\n",
      "Minibatch loss at step 28000: 0.322879\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 88.2%\n",
      "\n",
      "\n",
      "------Epoch n°9 ------\n",
      "\n",
      "Minibatch loss at step 30000: 0.325473\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.5%\n",
      "\n",
      "Minibatch loss at step 32000: 0.575603\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 88.4%\n",
      "\n",
      "\n",
      "------Epoch n°10 ------\n",
      "\n",
      "Minibatch loss at step 34000: 0.300804\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.6%\n",
      "\n",
      "Minibatch loss at step 36000: 0.317832\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.5%\n",
      "\n",
      "Test accuracy: 94.6%\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "n_step_one_epoch = int(size_train / batch_size) # Some images will not be used\n",
    "num_steps = int(n_epoch * n_step_one_epoch)\n",
    "t1 = time.time()   \n",
    "\n",
    "num_epoch = 1\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        if ( step % n_step_one_epoch == 0 ):\n",
    "            print('\\n------Epoch n°%d ------\\n' % num_epoch)\n",
    "            num_epoch +=1\n",
    "        # Simple offset for batch\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # Batch creation\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Feed dictionary\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        # Run optimizer, loss and train_prediction with the feeded batch\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 2000 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print()\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deep fully connected network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's build a more advanced fully connected network with re-usable code made of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Deep neural network with:\n",
    " - 3 hidden layers\n",
    " - Relu activation for hidden layers\n",
    " - Softmax loss\n",
    " - L2 regularization\n",
    " - RMSProp with fixed decay learning rate and momentum\n",
    " - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The network is not really tuned, to do it's easier with tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape,stddev,name):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev, name=name)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape, value, name):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(value, shape=shape, name=name)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nn_layer(input_tensor, input_dim, output_dim, keep_prob, layer_name, weight_stddev, bias_value ,act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer. \n",
    "    It does a matrix multiply, bias add, and then an activation function.\n",
    "    \"\"\"\n",
    "    weights = weight_variable([input_dim, output_dim], weight_stddev, 'weight_'+layer_name)\n",
    "    tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights)\n",
    "    biases = bias_variable([output_dim], bias_value, 'bias_'+layer_name)\n",
    "    preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "    activations = act(preactivate, name='activation')\n",
    "    dropped = tf.nn.dropout(activations, keep_prob)\n",
    "    return dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loss_function(labels, output_layer, coef_reg, reg='l2', coef_reg2=1e-3):\n",
    "    \"\"\" Loss function with softmax implementing L1,L2 and L1+L2 regularization\"\"\"\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=output_layer))\n",
    "    if (reg == 'l1'):\n",
    "        regularizer = tf.contrib.layers.l1_regularizer(scale=coef_reg)\n",
    "    elif (reg == 'l2'):\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=coef_reg)\n",
    "    else:\n",
    "        l1 = regularizer = tf.contrib.layers.l1_regularizer(scale=coef_reg)\n",
    "        l2 = regularizer = tf.contrib.layers.l2_regularizer(scale=coef_reg2)\n",
    "        regularizer = tf.contrib.layers.sum_regularizer([l1,l2])\n",
    "        \n",
    "    reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "    reg_loss = cross_entropy + reg_term\n",
    "    return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feed_dict(dataset, dropout_rate=1):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if (dataset == 'train'):\n",
    "        offset = (step * batch_size) % (size_train - batch_size)\n",
    "        data_batch = train_dataset[offset:(offset + batch_size), :]\n",
    "        label_batch = train_labels[offset:(offset + batch_size), :]\n",
    "        keep_prob = dropout_rate\n",
    "    elif (dataset == 'validation'):\n",
    "        data_batch = valid_dataset\n",
    "        label_batch = valid_labels\n",
    "        keep_prob = 1\n",
    "    elif (dataset == 'test'):\n",
    "        data_batch = test_dataset\n",
    "        label_batch = test_labels\n",
    "        keep_prob = 1\n",
    "    return {x: data_batch, y_: label_batch, k: keep_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data set parameters \n",
    "image_size = 28  # Pixel width and height.\n",
    "n_input = image_size*image_size # MNIST data input\n",
    "num_labels = 10 # Number of labels\n",
    "size_train = train_labels.shape[0]\n",
    "\n",
    "# Parameters of the model\n",
    "batch_size = 300\n",
    "size_hidden_layer = [1024,300,60]\n",
    "# Initialize weight with an std of sqrt(2/n) for Relu and 1/sqrt(n) for FC without relu\n",
    "std_weights = [np.sqrt(2.0/n_input),np.sqrt(2.0/size_hidden_layer[0]),\n",
    "               np.sqrt(2.0/size_hidden_layer[1]),1/(np.sqrt(size_hidden_layer[2]))]\n",
    "bias_value = 0.1\n",
    "learning_rate = 1e-4\n",
    "regul = 8e-4\n",
    "decay_rate = 0.95\n",
    "dropout_rate = 1\n",
    "momentum = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Count the number of steps taken\n",
    "    global_step = tf.Variable(0) \n",
    "    \n",
    "    # Placeholder for train, valid and test and the dropout rate\n",
    "    x = tf.placeholder(tf.float32, shape=[None, image_size * image_size], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, shape=[None, num_labels], name='y-input')\n",
    "    k = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Define the model\n",
    "    layer_1 = nn_layer(x, n_input, size_hidden_layer[0], k, \n",
    "                       'layer_1', std_weights[0], bias_value, tf.nn.relu)\n",
    "    layer_2 = nn_layer(layer_1, size_hidden_layer[0], size_hidden_layer[1], k, \n",
    "                       'layer_2', std_weights[1], bias_value, tf.nn.relu)\n",
    "    layer_3 = nn_layer(layer_2, size_hidden_layer[1], size_hidden_layer[2], k, \n",
    "                       'layer_3', std_weights[2], bias_value, tf.nn.relu)\n",
    "    y = nn_layer(layer_3, size_hidden_layer[2], num_labels, k, \n",
    "                 'layer_4', std_weights[3], bias_value, tf.identity)       \n",
    "    \n",
    "    # Loss function - Cross entropy + regularization   \n",
    "    reg_loss = loss_function(y_, y, regul, reg='l2')\n",
    "\n",
    "    # Optimizer    \n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=decay_rate, momentum=momentum\n",
    "                                         ).minimize(reg_loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the train, valid, and test.\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    accuracy = tf.cast(accuracy, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "------Epoch n°1 ------\n",
      "\n",
      "Minibatch loss at step 0: 3.220108\n",
      "Minibatch accuracy: 11.33% \n",
      "Validation accuracy: 11.64%\n",
      "\n",
      "Minibatch loss at step 1000: 0.987244\n",
      "Minibatch accuracy: 89.00% \n",
      "Validation accuracy: 87.07%\n",
      "\n",
      "\n",
      "------Epoch n°2 ------\n",
      "\n",
      "Minibatch loss at step 2000: 0.705047\n",
      "Minibatch accuracy: 87.67% \n",
      "Validation accuracy: 88.87%\n",
      "\n",
      "Minibatch loss at step 3000: 0.550241\n",
      "Minibatch accuracy: 91.00% \n",
      "Validation accuracy: 89.31%\n",
      "\n",
      "\n",
      "------Epoch n°3 ------\n",
      "\n",
      "Minibatch loss at step 4000: 0.578290\n",
      "Minibatch accuracy: 90.00% \n",
      "Validation accuracy: 89.64%\n",
      "\n",
      "\n",
      "------Epoch n°4 ------\n",
      "\n",
      "Minibatch loss at step 5000: 0.465187\n",
      "Minibatch accuracy: 90.33% \n",
      "Validation accuracy: 90.15%\n",
      "\n",
      "Minibatch loss at step 6000: 0.361800\n",
      "Minibatch accuracy: 92.67% \n",
      "Validation accuracy: 90.06%\n",
      "\n",
      "\n",
      "------Epoch n°5 ------\n",
      "\n",
      "Minibatch loss at step 7000: 0.431164\n",
      "Minibatch accuracy: 91.00% \n",
      "Validation accuracy: 90.30%\n",
      "\n",
      "\n",
      "------Epoch n°6 ------\n",
      "\n",
      "Minibatch loss at step 8000: 0.386352\n",
      "Minibatch accuracy: 92.00% \n",
      "Validation accuracy: 90.51%\n",
      "\n",
      "Minibatch loss at step 9000: 0.427455\n",
      "Minibatch accuracy: 90.33% \n",
      "Validation accuracy: 90.20%\n",
      "\n",
      "\n",
      "------Epoch n°7 ------\n",
      "\n",
      "Minibatch loss at step 10000: 0.323129\n",
      "Minibatch accuracy: 92.67% \n",
      "Validation accuracy: 90.65%\n",
      "\n",
      "\n",
      "------Epoch n°8 ------\n",
      "\n",
      "Minibatch loss at step 11000: 0.348757\n",
      "Minibatch accuracy: 92.67% \n",
      "Validation accuracy: 90.59%\n",
      "\n",
      "Minibatch loss at step 12000: 0.365307\n",
      "Minibatch accuracy: 91.67% \n",
      "Validation accuracy: 90.72%\n",
      "\n",
      "\n",
      "------Epoch n°9 ------\n",
      "\n",
      "Minibatch loss at step 13000: 0.369234\n",
      "Minibatch accuracy: 91.33% \n",
      "Validation accuracy: 90.78%\n",
      "\n",
      "Minibatch loss at step 14000: 0.374610\n",
      "Minibatch accuracy: 90.33% \n",
      "Validation accuracy: 90.80%\n",
      "\n",
      "\n",
      "------Epoch n°10 ------\n",
      "\n",
      "Minibatch loss at step 15000: 0.319854\n",
      "Minibatch accuracy: 93.33% \n",
      "Validation accuracy: 91.10%\n",
      "\n",
      "Test accuracy: 96.03%\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "n_step_one_epoch = int(size_train / batch_size)\n",
    "num_steps = int(n_epoch * n_step_one_epoch)\n",
    "interval_step_Train = 1000\n",
    "interval_step_Valid = 1000\n",
    "num_epoch = 1\n",
    "\n",
    "try:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Initialized\")\n",
    "        for step in range(num_steps):\n",
    "            if ( step % n_step_one_epoch == 0 ):\n",
    "                print('\\n------Epoch n°%d ------\\n' % num_epoch)\n",
    "                num_epoch +=1\n",
    "            _, l, acc = session.run([optimizer, reg_loss, accuracy], feed_dict=feed_dict('train',dropout_rate))\n",
    "            # Run optimizer, loss and train_prediction with the feeded batch \n",
    "            if (step % interval_step_Train == 0):\n",
    "                print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "                print(\"Minibatch accuracy: %.2f%% \" % (100*acc))\n",
    "            if (step % interval_step_Valid == 0):\n",
    "                print('Validation accuracy: %.2f%%' % (100*accuracy.eval(feed_dict=feed_dict('validation'))))\n",
    "                print()\n",
    "        print('Test accuracy: %.2f%%' % (100*accuracy.eval(feed_dict=feed_dict('test'))))\n",
    "except Exception as e:\n",
    "    print('An error occur in step', step, ':', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Deep fully connected network with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape,stddev):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape, value):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(value, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nn_layer(input_tensor, input_dim, output_dim, keep_prob, layer_name, weight_stddev, bias_value ,act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer. \n",
    "    It does a matrix multiply, bias add, and then an activation function.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('W'):\n",
    "            weights = weight_variable([input_dim, output_dim], weight_stddev)\n",
    "            variable_summaries(weights)\n",
    "            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights)\n",
    "        with tf.name_scope('b'):\n",
    "            biases = bias_variable([output_dim], bias_value)\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = act(preactivate, name='activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        dropped = tf.nn.dropout(activations, keep_prob)\n",
    "        tf.summary.histogram('activations_dropout', dropped)\n",
    "        return dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loss_function(labels, output_layer, coef_reg, reg='l2', coef_reg2=1e-3):\n",
    "    \"\"\" Loss function with softmax implementing L1,L2 and L1+L2 regularization\"\"\"\n",
    "    with tf.name_scope('loss_function'):\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=output_layer))\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "        if (reg == 'l1'):\n",
    "            regularizer = tf.contrib.layers.l1_regularizer(scale=coef_reg)\n",
    "        elif (reg == 'l2'):\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale=coef_reg)\n",
    "        else:\n",
    "            l1 = regularizer = tf.contrib.layers.l1_regularizer(scale=coef_reg)\n",
    "            l2 = regularizer = tf.contrib.layers.l2_regularizer(scale=coef_reg2)\n",
    "            regularizer = tf.contrib.layers.sum_regularizer([l1,l2])\n",
    "\n",
    "        reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "        with tf.name_scope('cross_entropy_regularization'):\n",
    "            reg_loss = cross_entropy + reg_term\n",
    "        tf.summary.scalar('cross_entropy_regularization', reg_loss)\n",
    "        return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feed_dict(dataset, dropout_rate=1):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if (dataset == 'train'):\n",
    "        offset = (step * batch_size) % (size_train - batch_size)\n",
    "        data_batch = train_dataset[offset:(offset + batch_size), :]\n",
    "        label_batch = train_labels[offset:(offset + batch_size), :]\n",
    "        keep_prob = dropout_rate\n",
    "    elif (dataset == 'validation'):\n",
    "        data_batch = valid_dataset\n",
    "        label_batch = valid_labels\n",
    "        keep_prob = 1\n",
    "    elif (dataset == 'test'):\n",
    "        data_batch = test_dataset\n",
    "        label_batch = test_labels\n",
    "        keep_prob = 1\n",
    "    return {x: data_batch, y_: label_batch, k: keep_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data set parameters \n",
    "image_size = 28  # Pixel width and height.\n",
    "n_input = image_size*image_size # MNIST data input\n",
    "num_labels = 10 # Number of labels\n",
    "size_train = train_labels.shape[0]\n",
    "\n",
    "# Parameters of the model\n",
    "batch_size = 300\n",
    "size_hidden_layer = [1024,300,60]\n",
    "# Initialize weight with an std of sqrt(2/n) for Relu and 1/sqrt(n) for FC without relu\n",
    "std_weights = [np.sqrt(2.0/n_input),np.sqrt(2.0/size_hidden_layer[0]),\n",
    "               np.sqrt(2.0/size_hidden_layer[1]),1/(np.sqrt(size_hidden_layer[2]))]\n",
    "bias_value = 0.1\n",
    "learning_rate = 1e-4\n",
    "regul = 8e-4\n",
    "decay_rate = 0.95\n",
    "dropout_rate = 1\n",
    "momentum = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Count the number of steps taken\n",
    "    global_step = tf.Variable(0)   \n",
    "    \n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, image_size * image_size], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, num_labels], name='y-input')\n",
    "        \n",
    "    with tf.name_scope('input_reshape'):\n",
    "        x_reshape = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input-image',x_reshape,10)\n",
    "\n",
    "    with tf.name_scope('dropout_rate'):    \n",
    "        k = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Define the model\n",
    "    layer_1 = nn_layer(x, n_input, size_hidden_layer[0], k, \n",
    "                       'FC_1', std_weights[0], bias_value, tf.nn.relu)\n",
    "    layer_2 = nn_layer(layer_1, size_hidden_layer[0], size_hidden_layer[1], k, \n",
    "                       'FC_2', std_weights[1], bias_value, tf.nn.relu)\n",
    "    layer_3 = nn_layer(layer_2, size_hidden_layer[1], size_hidden_layer[2], k, \n",
    "                       'FC_3', std_weights[2], bias_value, tf.nn.relu)\n",
    "    y = nn_layer(layer_3, size_hidden_layer[2], num_labels, k, \n",
    "                 'FC_4', std_weights[3], bias_value, tf.identity)       \n",
    "    \n",
    "    # Loss function - Cross entropy + regularization   \n",
    "    reg_loss = loss_function(y_, y, regul, reg='l2')\n",
    "\n",
    "    # Optimizer\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=decay_rate, momentum=momentum\n",
    "                                         ).minimize(reg_loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the train, valid, and test.\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        accuracy = tf.cast(accuracy, tf.float32)\n",
    "    acc_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all the summaries and write them out to /tmp/tensorflow/mnist/logs/mnist_with_summaries (by default)\n",
    "    merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "\n",
      "------Epoch n°1 ------\n",
      "\n",
      "\n",
      "------Epoch n°2 ------\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 2\n",
    "n_step_one_epoch = int(size_train / batch_size)\n",
    "num_steps = int(n_epoch * n_step_one_epoch)\n",
    "num_epoch = 1\n",
    "\n",
    "step_eval_Train = 5\n",
    "step_eval_Valid = 10\n",
    "step_eval_Test = 500\n",
    "step_save = 1000\n",
    "\n",
    "log_path='logs/1' # Need to change the log directory each time\n",
    "\n",
    "try:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(log_path+'/train', session.graph)\n",
    "        valid_writer = tf.summary.FileWriter(log_path+'/valid', session.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_path+'/test', session.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Starting\")\n",
    "        for step in range(num_steps):\n",
    "            # Write the Epoch\n",
    "            if ( step % n_step_one_epoch == 0 ):\n",
    "                print('\\n------Epoch n°%d ------\\n' % num_epoch)\n",
    "                num_epoch +=1\n",
    "            # Save Informations for Training\n",
    "            if (step % step_eval_Train == 0):\n",
    "                summary_train  = session.run(merged_summary, feed_dict=feed_dict('train',dropout_rate))\n",
    "                train_writer.add_summary(summary_train, step)\n",
    "            # Save Accuracy for Validation\n",
    "            if (step % step_eval_Valid == 0):\n",
    "                summary_valid = session.run(acc_sum, feed_dict=feed_dict('validation'))\n",
    "                valid_writer.add_summary(summary_valid, step)\n",
    "            # Save Accuracy for Testing\n",
    "            if (step % step_eval_Test == 0):\n",
    "                summary_test = session.run(acc_sum, feed_dict=feed_dict('test'))\n",
    "                test_writer.add_summary(summary_test, step)\n",
    "            if (step % step_eval_Test == 0):\n",
    "                saver.save(session, os.path.join(log_path, \"model.ckpt\"), step)\n",
    "            # Run optimizer            \n",
    "            session.run(optimizer, feed_dict=feed_dict('train',dropout_rate))\n",
    "        # Closing writers\n",
    "        train_writer.close()\n",
    "        valid_writer.close()\n",
    "        test_writer.close()\n",
    "        print(\"Finished\")\n",
    "except Exception as e:\n",
    "    print('An error occur in step', step, ':', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep fully connected network with Tensorboard using embedding visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape,stddev):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape, value):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(value, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nn_layer(input_tensor, input_dim, output_dim, keep_prob, layer_name, weight_stddev, bias_value ,act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer. \n",
    "    It does a matrix multiply, bias add, and then an activation function.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('W'):\n",
    "            weights = weight_variable([input_dim, output_dim], weight_stddev)\n",
    "            variable_summaries(weights)\n",
    "            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights)\n",
    "        with tf.name_scope('b'):\n",
    "            biases = bias_variable([output_dim], bias_value)\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = act(preactivate, name='activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        dropped = tf.nn.dropout(activations, keep_prob)\n",
    "        tf.summary.histogram('activations_dropout', dropped)\n",
    "        return dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loss_function(labels, output_layer, coef_reg, reg='l2', coef_reg2=1e-3):\n",
    "    \"\"\" Loss function with softmax implementing L1,L2 and L1+L2 regularization\"\"\"\n",
    "    with tf.name_scope('loss_function'):\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=output_layer))\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "        if (reg == 'l1'):\n",
    "            regularizer = tf.contrib.layers.l1_regularizer(scale=coef_reg)\n",
    "        elif (reg == 'l2'):\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale=coef_reg)\n",
    "        else:\n",
    "            l1 = regularizer = tf.contrib.layers.l1_regularizer(scale=coef_reg)\n",
    "            l2 = regularizer = tf.contrib.layers.l2_regularizer(scale=coef_reg2)\n",
    "            regularizer = tf.contrib.layers.sum_regularizer([l1,l2])\n",
    "\n",
    "        reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "        with tf.name_scope('cross_entropy_regularization'):\n",
    "            reg_loss = cross_entropy + reg_term\n",
    "        tf.summary.scalar('cross_entropy_regularization', reg_loss)\n",
    "        return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feed_dict(dataset, dropout_rate=1):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if (dataset == 'train'):\n",
    "        offset = (step * batch_size) % (size_train - batch_size)\n",
    "        data_batch = train_dataset[offset:(offset + batch_size), :]\n",
    "        label_batch = train_labels[offset:(offset + batch_size), :]\n",
    "        keep_prob = dropout_rate\n",
    "    elif (dataset == 'validation'):\n",
    "        data_batch = valid_dataset\n",
    "        label_batch = valid_labels\n",
    "        keep_prob = 1\n",
    "    elif (dataset == 'test'):\n",
    "        data_batch = test_dataset\n",
    "        label_batch = test_labels\n",
    "        keep_prob = 1\n",
    "    return {x: data_batch, y_: label_batch, k: keep_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_tsv(index_label, path_to_save):\n",
    "    with open(path_to_save+'/tsv.tsv','w') as f:\n",
    "        f.write(\"Index\\tLabel\\n\")\n",
    "        for index,label in enumerate(index_label):\n",
    "            f.write(\"%s\\t%s\\n\" % (index,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Taken from: https://github.com/tensorflow/tensorflow/issues/6322\n",
    "def images_to_sprite(data, path_to_save):\n",
    "    \"\"\"Creates the sprite image along with any necessary padding\n",
    "\n",
    "    Args:\n",
    "      data: NxHxW[x3] tensor containing the images.\n",
    "\n",
    "    Returns:\n",
    "      data: Properly shaped HxWx3 image with any necessary padding.\n",
    "    \"\"\"\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.tile(data[...,np.newaxis], (1,1,1,3))\n",
    "    data = data.astype(np.float32)\n",
    "    min = np.min(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\n",
    "    max = np.max(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\n",
    "    # Inverting the colors seems to look better for MNIST or NOTMNIST\n",
    "    data = 1 - data\n",
    "\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, 0),\n",
    "            (0, 0)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant',\n",
    "            constant_values=0)\n",
    "    # Tile the individual thumbnails into an image.\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3)\n",
    "            + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    imsave(path_to_save+'/sprite.png', data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data set parameters \n",
    "image_size = 28  # Pixel width and height.\n",
    "n_input = image_size*image_size # MNIST data input\n",
    "num_labels = 10 # Number of labels\n",
    "size_train = train_labels.shape[0]\n",
    "\n",
    "# Parameters of the model\n",
    "batch_size = 300\n",
    "size_hidden_layer = [1024,300,60]\n",
    "# Initialize weight with an std of sqrt(2/n) for Relu and 1/sqrt(n) for FC without relu\n",
    "std_weights = [np.sqrt(2.0/n_input),np.sqrt(2.0/size_hidden_layer[0]),\n",
    "               np.sqrt(2.0/size_hidden_layer[1]),1/(np.sqrt(size_hidden_layer[2]))]\n",
    "bias_value = 0.1\n",
    "learning_rate = 1e-4\n",
    "regul = 8e-4\n",
    "decay_rate = 0.95\n",
    "dropout_rate = 1\n",
    "momentum = 0.5\n",
    "\n",
    "log_path='logs/1'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Count the number of steps taken\n",
    "    global_step = tf.Variable(0)   \n",
    "    \n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, image_size * image_size], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, num_labels], name='y-input')\n",
    "        \n",
    "    with tf.name_scope('input_reshape'):\n",
    "        x_reshape = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input-image',x_reshape,10)\n",
    "\n",
    "    with tf.name_scope('dropout_rate'):    \n",
    "        k = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Define the model\n",
    "    layer_1 = nn_layer(x, n_input, size_hidden_layer[0], k, \n",
    "                       'FC_1', std_weights[0], bias_value, tf.nn.relu)\n",
    "    layer_2 = nn_layer(layer_1, size_hidden_layer[0], size_hidden_layer[1], k, \n",
    "                       'FC_2', std_weights[1], bias_value, tf.nn.relu)\n",
    "    layer_3 = nn_layer(layer_2, size_hidden_layer[1], size_hidden_layer[2], k, \n",
    "                       'FC_3', std_weights[2], bias_value, tf.nn.relu)\n",
    "    y = nn_layer(layer_3, size_hidden_layer[2], num_labels, k, \n",
    "                 'FC_4', std_weights[3], bias_value, tf.identity)       \n",
    "    \n",
    "    # Loss function - Cross entropy + regularization   \n",
    "    reg_loss = loss_function(y_, y, regul, reg='l2')\n",
    "\n",
    "    # Optimizer\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=decay_rate, momentum=momentum\n",
    "                                         ).minimize(reg_loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the train, valid, and test.\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        accuracy = tf.cast(accuracy, tf.float32)\n",
    "        acc_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all the summaries\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Part for embedding visualization\n",
    "    embedding_writer = tf.summary.FileWriter(log_path)\n",
    "    number_of_image = test_labels.shape[0]\n",
    "    embedding_input = layer_3\n",
    "    embedding_size = size_hidden_layer[2]\n",
    "    embedding = tf.Variable(tf.zeros([number_of_image, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = log_path +'/sprite.png'\n",
    "    embedding_config.metadata_path = log_path + '/tsv.tsv'\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(embedding_writer, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "\n",
      "------Epoch n°1 ------\n",
      "\n",
      "\n",
      "------Epoch n°2 ------\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 2\n",
    "n_step_one_epoch = int(size_train / batch_size)\n",
    "num_steps = int(n_epoch * n_step_one_epoch)\n",
    "num_epoch = 1\n",
    "\n",
    "step_eval_Train = 5\n",
    "step_eval_Valid = 10\n",
    "step_eval_Test = 500\n",
    "step_save = 1000\n",
    "\n",
    "# Prepare for embedding visualization\n",
    "create_tsv(test_labels_raw,log_path)\n",
    "images_to_sprite(test_dataset_raw,log_path)\n",
    "\n",
    "try:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(log_path+'/train', session.graph)\n",
    "        valid_writer = tf.summary.FileWriter(log_path+'/valid', session.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_path+'/test', session.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        embedding_writer.add_graph(session.graph)\n",
    "        \n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Starting\")\n",
    "        for step in range(num_steps):\n",
    "            # Write the Epoch\n",
    "            if ( step % n_step_one_epoch == 0 ):\n",
    "                print('\\n------Epoch n°%d ------\\n' % num_epoch)\n",
    "                num_epoch +=1\n",
    "            # Save Informations for Training\n",
    "            if (step % step_eval_Train == 0):\n",
    "                summary_train  = session.run(merged_summary, feed_dict=feed_dict('train',dropout_rate))\n",
    "                train_writer.add_summary(summary_train, step)\n",
    "            # Save Accuracy for Validation\n",
    "            if (step % step_eval_Valid == 0):\n",
    "                summary_valid = session.run(acc_sum, feed_dict=feed_dict('validation'))\n",
    "                valid_writer.add_summary(summary_valid, step)\n",
    "            # Save Accuracy for Testing\n",
    "            if (step % step_eval_Test == 0):\n",
    "                summary_test = session.run(acc_sum, feed_dict=feed_dict('test'))\n",
    "                test_writer.add_summary(summary_test, step)\n",
    "            if (step % step_save == 0):\n",
    "                session.run(assignment,feed_dict=feed_dict('test'))\n",
    "                saver.save(session, os.path.join(log_path, \"model.ckpt\"), step)\n",
    "            # Run optimizer            \n",
    "            session.run(optimizer, feed_dict=feed_dict('train',dropout_rate))\n",
    "        # Closing writers\n",
    "        train_writer.close()\n",
    "        valid_writer.close()\n",
    "        test_writer.close()\n",
    "        print(\"Finished\")\n",
    "except Exception as e:\n",
    "    print('An error occur in step', step, ':', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Deep fully connected network with Tensorboard using embedding visualization and restore model to continue learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imsave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape,stddev):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def bias_variable(shape, value):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(value, shape=shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def nn_layer(input_tensor, input_dim, output_dim, keep_prob, layer_name, weight_stddev, bias_value ,act=tf.nn.relu):\n",
    "    \"\"\"Reusable code for making a simple neural net layer. \n",
    "    It does a matrix multiply, bias add, and then an activation function.\n",
    "    \"\"\"\n",
    "    with tf.name_scope(layer_name):\n",
    "        with tf.name_scope('W'):\n",
    "            weights = weight_variable([input_dim, output_dim], weight_stddev)\n",
    "            variable_summaries(weights)\n",
    "            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, weights)\n",
    "        with tf.name_scope('b'):\n",
    "            biases = bias_variable([output_dim], bias_value)\n",
    "            variable_summaries(biases)\n",
    "        with tf.name_scope('Wx_plus_b'):\n",
    "            preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "            tf.summary.histogram('pre_activations', preactivate)\n",
    "        activations = act(preactivate, name='activation')\n",
    "        tf.summary.histogram('activations', activations)\n",
    "        dropped = tf.nn.dropout(activations, keep_prob)\n",
    "        tf.summary.histogram('activations_dropout', dropped)\n",
    "        return dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def loss_function(labels, output_layer, coef_reg, reg='l2', coef_reg2=1e-3):\n",
    "    \"\"\" Loss function with softmax implementing L1,L2 and L1+L2 regularization\"\"\"\n",
    "    with tf.name_scope('loss_function'):\n",
    "        with tf.name_scope('cross_entropy'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=output_layer))\n",
    "        tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "        if (reg == 'l1'):\n",
    "            regularizer = tf.contrib.layers.l1_regularizer(scale=coef_reg)\n",
    "        elif (reg == 'l2'):\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(scale=coef_reg)\n",
    "        else:\n",
    "            l1 = regularizer = tf.contrib.layers.l1_regularizer(scale=coef_reg)\n",
    "            l2 = regularizer = tf.contrib.layers.l2_regularizer(scale=coef_reg2)\n",
    "            regularizer = tf.contrib.layers.sum_regularizer([l1,l2])\n",
    "\n",
    "        reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\n",
    "        with tf.name_scope('cross_entropy_regularization'):\n",
    "            reg_loss = cross_entropy + reg_term\n",
    "        tf.summary.scalar('cross_entropy_regularization', reg_loss)\n",
    "        return reg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feed_dict(dataset, dropout_rate=1):\n",
    "    \"\"\"Make a TensorFlow feed_dict: maps data onto Tensor placeholders.\"\"\"\n",
    "    if (dataset == 'train'):\n",
    "        offset = (step * batch_size) % (size_train - batch_size)\n",
    "        data_batch = train_dataset[offset:(offset + batch_size), :]\n",
    "        label_batch = train_labels[offset:(offset + batch_size), :]\n",
    "        keep_prob = dropout_rate\n",
    "    elif (dataset == 'validation'):\n",
    "        data_batch = valid_dataset\n",
    "        label_batch = valid_labels\n",
    "        keep_prob = 1\n",
    "    elif (dataset == 'test'):\n",
    "        data_batch = test_dataset\n",
    "        label_batch = test_labels\n",
    "        keep_prob = 1\n",
    "    return {x: data_batch, y_: label_batch, k: keep_prob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var):\n",
    "    \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_tsv(index_label, path_to_save):\n",
    "    with open(path_to_save+'/tsv.tsv','w') as f:\n",
    "        f.write(\"Index\\tLabel\\n\")\n",
    "        for index,label in enumerate(index_label):\n",
    "            f.write(\"%s\\t%s\\n\" % (index,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Taken from: https://github.com/tensorflow/tensorflow/issues/6322\n",
    "def images_to_sprite(data, path_to_save):\n",
    "    \"\"\"Creates the sprite image along with any necessary padding\n",
    "\n",
    "    Args:\n",
    "      data: NxHxW[x3] tensor containing the images.\n",
    "\n",
    "    Returns:\n",
    "      data: Properly shaped HxWx3 image with any necessary padding.\n",
    "    \"\"\"\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.tile(data[...,np.newaxis], (1,1,1,3))\n",
    "    data = data.astype(np.float32)\n",
    "    min = np.min(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\n",
    "    max = np.max(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\n",
    "    # Inverting the colors seems to look better for MNIST\n",
    "    data = 1 - data\n",
    "\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, 0),\n",
    "            (0, 0)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant',\n",
    "            constant_values=0)\n",
    "    # Tile the individual thumbnails into an image.\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3)\n",
    "            + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    imsave(path_to_save+'/sprite.png', data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data set parameters \n",
    "image_size = 28  # Pixel width and height.\n",
    "n_input = image_size*image_size # MNIST data input\n",
    "num_labels = 10 # Number of labels\n",
    "size_train = train_labels.shape[0]\n",
    "\n",
    "# Parameters of the model\n",
    "batch_size = 300\n",
    "size_hidden_layer = [1024,300,60]\n",
    "# Initialize weight with an std of sqrt(2/n) for Relu and 1/sqrt(n) for FC without relu\n",
    "std_weights = [np.sqrt(2.0/n_input),np.sqrt(2.0/size_hidden_layer[0]),\n",
    "               np.sqrt(2.0/size_hidden_layer[1]),1/(np.sqrt(size_hidden_layer[2]))]\n",
    "bias_value = 0.1\n",
    "learning_rate = 1e-4\n",
    "regul = 8e-4\n",
    "decay_rate = 0.95\n",
    "dropout_rate = 1\n",
    "momentum = 0.5\n",
    "\n",
    "# Log directories\n",
    "log_path='logs/8'\n",
    "train_path = log_path+'/train'\n",
    "valid_path = log_path+'/valid'\n",
    "test_path = log_path+'/test'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Count the number of steps taken\n",
    "    global_step = tf.Variable(0)   \n",
    "    \n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, image_size * image_size], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, num_labels], name='y-input')\n",
    "        \n",
    "    with tf.name_scope('input_reshape'):\n",
    "        x_reshape = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input-image',x_reshape,10)\n",
    "\n",
    "    with tf.name_scope('dropout_rate'):    \n",
    "        k = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Define the model\n",
    "    layer_1 = nn_layer(x, n_input, size_hidden_layer[0], k, \n",
    "                       'FC_1', std_weights[0], bias_value, tf.nn.relu)\n",
    "    layer_2 = nn_layer(layer_1, size_hidden_layer[0], size_hidden_layer[1], k, \n",
    "                       'FC_2', std_weights[1], bias_value, tf.nn.relu)\n",
    "    layer_3 = nn_layer(layer_2, size_hidden_layer[1], size_hidden_layer[2], k, \n",
    "                       'FC_3', std_weights[2], bias_value, tf.nn.relu)\n",
    "    y = nn_layer(layer_3, size_hidden_layer[2], num_labels, k, \n",
    "                 'FC_4', std_weights[3], bias_value, tf.identity)       \n",
    "    \n",
    "    # Loss function - Cross entropy + regularization   \n",
    "    reg_loss = loss_function(y_, y, regul, reg='l2')\n",
    "\n",
    "    # Optimizer\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=decay_rate, momentum=momentum\n",
    "                                         ).minimize(reg_loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the train, valid, and test.\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        accuracy = tf.cast(accuracy, tf.float32)\n",
    "        acc_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all the summaries\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    \n",
    "    # Logs and save\n",
    "    with tf.name_scope('log_save'):\n",
    "        train_writer = tf.summary.FileWriter(train_path)\n",
    "        valid_writer = tf.summary.FileWriter(valid_path)\n",
    "        test_writer = tf.summary.FileWriter(test_path)\n",
    "        embed_writter = tf.summary.FileWriter(log_path)\n",
    "        #saver = tf.train.Saver()\n",
    "    \n",
    "    # Part for embedding visualization\n",
    "    #with tf.name_scope('embedding'):\n",
    "    number_of_image_for_embbeding = test_labels.shape[0]\n",
    "    embedding_input = layer_3\n",
    "    embedding_size = size_hidden_layer[2]\n",
    "    embedding = tf.Variable(tf.zeros([number_of_image_for_embbeding, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = log_path +'/sprite.png'\n",
    "    embedding_config.metadata_path = log_path + '/tsv.tsv'\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(embed_writter, config)\n",
    "    \n",
    "    # Initialize variables\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "\n",
      "------Epoch n°1 ------\n",
      "\n",
      "saved model logs/5\\model.ckpt-0\n",
      "saved model logs/5\\model.ckpt-1000\n",
      "\n",
      "------Epoch n°2 ------\n",
      "\n",
      "saved model logs/5\\model.ckpt-2000\n",
      "saved model logs/5\\model.ckpt-3000\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 2\n",
    "n_step_one_epoch = int(size_train / batch_size)\n",
    "num_steps = int(n_epoch * n_step_one_epoch)\n",
    "num_epoch = 1\n",
    "\n",
    "step_eval_Train = 5\n",
    "step_eval_Valid = 10\n",
    "step_eval_Test = 500\n",
    "step_save = 1000\n",
    "\n",
    "# Prepare for embedding visualization\n",
    "create_tsv(test_labels_raw,log_path)\n",
    "images_to_sprite(test_dataset_raw,log_path)\n",
    "\n",
    "try:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        # Initialize variables\n",
    "        #session.run(init)\n",
    "        # Logs\n",
    "        train_writer.add_graph(session.graph)\n",
    "        valid_writer.add_graph(session.graph)\n",
    "        test_writer.add_graph(session.graph)\n",
    "        embed_writter.add_graph(session.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        tf.global_variables_initializer().run()\n",
    "        print(\"Starting\")\n",
    "        for step in range(num_steps):\n",
    "            # Write the Epoch\n",
    "            if ( step % n_step_one_epoch == 0 ):\n",
    "                print('\\n------Epoch n°%d ------\\n' % num_epoch)\n",
    "                num_epoch +=1\n",
    "            # Save Informations for Training\n",
    "            if (step % step_eval_Train == 0):\n",
    "                summary_train  = session.run(merged_summary, feed_dict=feed_dict('train',dropout_rate))\n",
    "                train_writer.add_summary(summary_train, step)\n",
    "            # Save Accuracy for Validation\n",
    "            if (step % step_eval_Valid == 0):\n",
    "                summary_valid = session.run(acc_sum, feed_dict=feed_dict('validation'))\n",
    "                valid_writer.add_summary(summary_valid, step)\n",
    "            # Save Accuracy for Testing\n",
    "            if (step % step_eval_Test == 0):\n",
    "                summary_test = session.run(acc_sum, feed_dict=feed_dict('test'))\n",
    "                test_writer.add_summary(summary_test, step)\n",
    "            if (step % step_save == 0):\n",
    "                session.run(assignment,feed_dict=feed_dict('test'))\n",
    "                saved_model = saver.save(session, os.path.join(log_path, \"model.ckpt\"), step)\n",
    "                print('saved model',saved_model)\n",
    "            # Run optimizer            \n",
    "            session.run(optimizer, feed_dict=feed_dict('train',dropout_rate))\n",
    "        # Closing writers\n",
    "        train_writer.close()\n",
    "        valid_writer.close()\n",
    "        test_writer.close()\n",
    "        embed_writter.close()\n",
    "        print(\"Finished\")\n",
    "except Exception as e:\n",
    "    print('An error occur in step', step, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Data set parameters \n",
    "image_size = 28  # Pixel width and height.\n",
    "n_input = image_size*image_size # MNIST data input\n",
    "num_labels = 10 # Number of labels\n",
    "size_train = train_labels.shape[0]\n",
    "\n",
    "# Parameters of the model\n",
    "batch_size = 300\n",
    "size_hidden_layer = [1024,300,60]\n",
    "# Initialize weight with an std of sqrt(2/n) for Relu and 1/sqrt(n) for FC without relu\n",
    "std_weights = [np.sqrt(2.0/n_input),np.sqrt(2.0/size_hidden_layer[0]),\n",
    "               np.sqrt(2.0/size_hidden_layer[1]),1/(np.sqrt(size_hidden_layer[2]))]\n",
    "bias_value = 0.1\n",
    "learning_rate = 1e-4\n",
    "regul = 8e-4\n",
    "decay_rate = 0.95\n",
    "dropout_rate = 1\n",
    "momentum = 0.5\n",
    "\n",
    "# Log directories\n",
    "log_path='logs/9'\n",
    "train_path = log_path+'/train'\n",
    "valid_path = log_path+'/valid'\n",
    "test_path = log_path+'/test'\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Count the number of steps taken\n",
    "    global_step = tf.Variable(0)   \n",
    "    \n",
    "    with tf.name_scope('input'):\n",
    "        x = tf.placeholder(tf.float32, shape=[None, image_size * image_size], name='x-input')\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, num_labels], name='y-input')\n",
    "        \n",
    "    with tf.name_scope('input_reshape'):\n",
    "        x_reshape = tf.reshape(x, [-1, 28, 28, 1])\n",
    "        tf.summary.image('input-image',x_reshape,10)\n",
    "\n",
    "    with tf.name_scope('dropout_rate'):    \n",
    "        k = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Define the model\n",
    "    layer_1 = nn_layer(x, n_input, size_hidden_layer[0], k, \n",
    "                       'FC_1', std_weights[0], bias_value, tf.nn.relu)\n",
    "    layer_2 = nn_layer(layer_1, size_hidden_layer[0], size_hidden_layer[1], k, \n",
    "                       'FC_2', std_weights[1], bias_value, tf.nn.relu)\n",
    "    layer_3 = nn_layer(layer_2, size_hidden_layer[1], size_hidden_layer[2], k, \n",
    "                       'FC_3', std_weights[2], bias_value, tf.nn.relu)\n",
    "    y = nn_layer(layer_3, size_hidden_layer[2], num_labels, k, \n",
    "                 'FC_4', std_weights[3], bias_value, tf.identity)       \n",
    "    \n",
    "    # Loss function - Cross entropy + regularization   \n",
    "    reg_loss = loss_function(y_, y, regul, reg='l2')\n",
    "\n",
    "    # Optimizer\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=decay_rate, momentum=momentum\n",
    "                                         ).minimize(reg_loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions for the train, valid, and test.\n",
    "    with tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        accuracy = tf.cast(accuracy, tf.float32)\n",
    "        acc_sum = tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    # Merge all the summaries\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    \n",
    "    # Logs and save\n",
    "    with tf.name_scope('log_save'):\n",
    "        train_writer = tf.summary.FileWriter(train_path)\n",
    "        valid_writer = tf.summary.FileWriter(valid_path)\n",
    "        test_writer = tf.summary.FileWriter(test_path)\n",
    "        embed_writter = tf.summary.FileWriter(log_path)\n",
    "        #saver = tf.train.Saver()\n",
    "    \n",
    "    # Part for embedding visualization\n",
    "    #with tf.name_scope('embedding'):\n",
    "    number_of_image_for_embbeding = test_labels.shape[0]\n",
    "    embedding_input = layer_3\n",
    "    embedding_size = size_hidden_layer[2]\n",
    "    embedding = tf.Variable(tf.zeros([number_of_image_for_embbeding, embedding_size]), name=\"test_embedding\")\n",
    "    assignment = embedding.assign(embedding_input)\n",
    "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
    "    embedding_config = config.embeddings.add()\n",
    "    embedding_config.tensor_name = embedding.name\n",
    "    embedding_config.sprite.image_path = log_path +'/sprite.png'\n",
    "    embedding_config.metadata_path = log_path + '/tsv.tsv'\n",
    "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
    "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(embed_writter, config)\n",
    "    \n",
    "    # Initialize variables\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting\n",
      "\n",
      "------Epoch n°1 ------\n",
      "\n",
      "\n",
      "------Epoch n°2 ------\n",
      "\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 2\n",
    "n_step_one_epoch = int(size_train / batch_size)\n",
    "num_steps = int(n_epoch * n_step_one_epoch)\n",
    "num_epoch = 1\n",
    "\n",
    "step_eval_Train = 5\n",
    "step_eval_Valid = 10\n",
    "step_eval_Test = 500\n",
    "step_save = 1000\n",
    "\n",
    "# Prepare for embedding visualization\n",
    "create_tsv(test_labels_raw,log_path)\n",
    "images_to_sprite(test_dataset_raw,log_path)\n",
    "\n",
    "try:\n",
    "    with tf.Session(graph=graph) as session:\n",
    "        \n",
    "        train_writer.add_graph(session.graph)\n",
    "        valid_writer.add_graph(session.graph)\n",
    "        test_writer.add_graph(session.graph)\n",
    "        embed_writter.add_graph(session.graph)\n",
    "        saver = tf.train.Saver()\n",
    "        tf.global_variables_initializer().run()\n",
    "        load_path = saver.restore(session, saved_model) \n",
    "        \n",
    "        print(\"Starting\")\n",
    "        for step in range(num_steps):\n",
    "            # Write the Epoch\n",
    "            if ( step % n_step_one_epoch == 0 ):\n",
    "                print('\\n------Epoch n°%d ------\\n' % num_epoch)\n",
    "                num_epoch +=1\n",
    "            # Save Informations for Training\n",
    "            if (step % step_eval_Train == 0):\n",
    "                summary_train  = session.run(merged_summary, feed_dict=feed_dict('train',dropout_rate))\n",
    "                train_writer.add_summary(summary_train, step)\n",
    "            # Save Accuracy for Validation\n",
    "            if (step % step_eval_Valid == 0):\n",
    "                summary_valid = session.run(acc_sum, feed_dict=feed_dict('validation'))\n",
    "                valid_writer.add_summary(summary_valid, step)\n",
    "            # Save Accuracy for Testing\n",
    "            if (step % step_eval_Test == 0):\n",
    "                summary_test = session.run(acc_sum, feed_dict=feed_dict('test'))\n",
    "                test_writer.add_summary(summary_test, step)\n",
    "            if (step % step_save == 0):\n",
    "                session.run(assignment,feed_dict=feed_dict('test'))\n",
    "                saver.save(session, os.path.join(log_path, \"model.ckpt\"), step)\n",
    "            # Run optimizer            \n",
    "            session.run(optimizer, feed_dict=feed_dict('train',dropout_rate))\n",
    "        # Closing writers\n",
    "        train_writer.close()\n",
    "        valid_writer.close()\n",
    "        test_writer.close()\n",
    "        embed_writter.close()\n",
    "        print(\"Finished\")\n",
    "except Exception as e:\n",
    "    print('An error occur in step', step, ':', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
